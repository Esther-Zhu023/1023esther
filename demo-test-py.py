#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ MCP æœåŠ¡å™¨æ¼”ç¤ºæµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹çš„å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. ç½‘ç»œæœç´¢
2. æ–‡æ¡£è·å–å’Œåˆ†æ
3. ç ”ç©¶æ‘˜è¦ç”Ÿæˆ
4. çŸ¥è¯†åº“ç®¡ç†
5. ç›¸å…³ç ”ç©¶æ£€ç´¢

è¿è¡Œæ–¹æ³•:
python demo_test.py
"""

import os
import sys
import json
import time
import asyncio
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.mcp_server import ResearchAssistantMCPServer
from src.rag.vector_store import VectorStore
from src.agents.search_agent import SearchAgent
from src.agents.analysis_agent import AnalysisAgent
from src.agents.synthesis_agent import SynthesisAgent
from src.tools.knowledge_base import KnowledgeBase

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
os.environ.setdefault("OPENAI_API_KEY", "your_openai_api_key_here")
os.environ.setdefault("GROQ_API_KEY", "your_groq_api_key_here")
os.environ.setdefault("SEARCH_API_KEY", "your_search_api_key_here")
os.environ.setdefault("LOCAL_STORAGE_PATH", "./demo_data/vector_store")
os.environ.setdefault("KB_INDEX_PATH", "./demo_data/knowledge_base_index.json")

class DemoTestRunner:
    """æ¼”ç¤ºæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        print("ğŸš€ åˆå§‹åŒ–æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹æ¼”ç¤ºæµ‹è¯•...")
        
        # åˆ›å»ºæ¼”ç¤ºæ•°æ®ç›®å½•
        os.makedirs("./demo_data/vector_store", exist_ok=True)
        os.makedirs("./demo_data/logs", exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.vector_store = VectorStore()
        self.search_agent = SearchAgent(self.vector_store)
        self.analysis_agent = AnalysisAgent(self.vector_store)
        self.synthesis_agent = SynthesisAgent(self.vector_store)
        self.knowledge_base = KnowledgeBase(self.vector_store)
        
        print("âœ… åˆå§‹åŒ–å®Œæˆï¼")
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤ºæµ‹è¯•"""
        print("\n" + "="*60)
        print("ğŸ§ª å¼€å§‹æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹åŠŸèƒ½æ¼”ç¤ºæµ‹è¯•")
        print("="*60)
        
        # æµ‹è¯•åˆ—è¡¨
        tests = [
            ("æœç´¢ä»£ç†æµ‹è¯•", self.test_search_agent),
            ("åˆ†æä»£ç†æµ‹è¯•", self.test_analysis_agent),
            ("åˆæˆä»£ç†æµ‹è¯•", self.test_synthesis_agent),
            ("çŸ¥è¯†åº“æµ‹è¯•", self.test_knowledge_base),
            ("å®Œæ•´ç ”ç©¶æµç¨‹æµ‹è¯•", self.test_complete_research_workflow)
        ]
        
        results = []
        
        for test_name, test_func in tests:
            print(f"\nğŸ“‹ {test_name}")
            print("-" * 40)
            
            try:
                start_time = time.time()
                result = test_func()
                end_time = time.time()
                
                print(f"âœ… {test_name} å®Œæˆ (è€—æ—¶: {end_time - start_time:.2f}ç§’)")
                results.append({"test": test_name, "status": "success", "time": end_time - start_time})
                
            except Exception as e:
                print(f"âŒ {test_name} å¤±è´¥: {e}")
                results.append({"test": test_name, "status": "failed", "error": str(e)})
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
        self.show_test_summary(results)
    
    def test_search_agent(self):
        """æµ‹è¯•æœç´¢ä»£ç†"""
        print("ğŸ” æµ‹è¯•ç½‘ç»œæœç´¢åŠŸèƒ½...")
        
        # æµ‹è¯•æŸ¥è¯¢
        query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
        
        # æ‰§è¡Œæœç´¢
        search_results = self.search_agent.search_web(query, max_results=3)
        
        print(f"æœç´¢æŸ¥è¯¢: {query}")
        print(f"æ‰¾åˆ° {len(search_results)} æ¡ç»“æœ:")
        
        for i, result in enumerate(search_results, 1):
            print(f"  {i}. {result.get('title', 'æ— æ ‡é¢˜')}")
            print(f"     URL: {result.get('url', 'æ— URL')}")
            print(f"     æ‘˜è¦: {result.get('snippet', 'æ— æ‘˜è¦')[:100]}...")
        
        # æµ‹è¯•æ–‡æ¡£è·å–
        if search_results:
            print(f"\nğŸ“„ æµ‹è¯•æ–‡æ¡£è·å–åŠŸèƒ½...")
            first_url = search_results[0].get('url')
            if first_url:
                try:
                    document = self.search_agent.fetch_document(first_url)
                    print(f"æˆåŠŸè·å–æ–‡æ¡£: {first_url}")
                    print(f"å†…å®¹é•¿åº¦: {len(document.get('content', ''))}")
                    print(f"å†…å®¹é¢„è§ˆ: {document.get('content', '')[:200]}...")
                except Exception as e:
                    print(f"æ–‡æ¡£è·å–å¤±è´¥: {e}")
        
        return {"search_results": len(search_results)}
    
    def test_analysis_agent(self):
        """æµ‹è¯•åˆ†æä»£ç†"""
        print("ğŸ”¬ æµ‹è¯•å†…å®¹åˆ†æåŠŸèƒ½...")
        
        # æµ‹è¯•å†…å®¹
        test_content = """
        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚ä¸»è¦åº”ç”¨åŒ…æ‹¬ï¼š
        1. åŒ»å­¦å½±åƒè¯Šæ–­ï¼šAIå¯ä»¥å¸®åŠ©åŒ»ç”Ÿæ›´å‡†ç¡®åœ°è¯†åˆ«Xå…‰ã€CTå’ŒMRIå›¾åƒä¸­çš„å¼‚å¸¸ã€‚
        2. è¯ç‰©å‘ç°ï¼šæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åŠ é€Ÿæ–°è¯çš„å‘ç°å’Œå¼€å‘è¿‡ç¨‹ã€‚
        3. ä¸ªæ€§åŒ–æ²»ç–—ï¼šAIå¯ä»¥æ ¹æ®æ‚£è€…çš„åŸºå› ä¿¡æ¯å’ŒåŒ»ç–—å†å²æä¾›ä¸ªæ€§åŒ–çš„æ²»ç–—æ–¹æ¡ˆã€‚
        4. é¢„æµ‹æ€§åŒ»ç–—ï¼šé€šè¿‡åˆ†æå¤§é‡åŒ»ç–—æ•°æ®ï¼ŒAIå¯ä»¥é¢„æµ‹ç–¾ç—…çš„å‘ç”Ÿå’Œè¿›å±•ã€‚
        5. æ™ºèƒ½è¯Šæ–­ï¼šAIåŠ©æ‰‹å¯ä»¥å¸®åŠ©åŒ»ç”Ÿè¿›è¡Œåˆæ­¥è¯Šæ–­å’Œæ²»ç–—å»ºè®®ã€‚
        """
        
        query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
        
        # æ‰§è¡Œå†…å®¹åˆ†æ
        analysis_result = self.analysis_agent.analyze_content(test_content, query)
        
        print(f"åˆ†ææŸ¥è¯¢: {query}")
        print(f"ç›¸å…³æ€§åˆ†æ•°: {analysis_result.get('relevance', 0):.2f}")
        print(f"æ¨ç†: {analysis_result.get('reasoning', 'æ— æ¨ç†')}")
        
        insights = analysis_result.get('insights', [])
        print(f"æå–çš„æ´è§ ({len(insights)}æ¡):")
        for i, insight in enumerate(insights, 1):
            print(f"  {i}. {insight}")
        
        # æµ‹è¯•å…³é”®ä¿¡æ¯æå–
        print(f"\nğŸ”‘ æµ‹è¯•å…³é”®ä¿¡æ¯æå–...")
        topics = ["åŒ»å­¦å½±åƒ", "è¯ç‰©å‘ç°", "ä¸ªæ€§åŒ–æ²»ç–—"]
        key_info = self.analysis_agent.extract_key_information(test_content, topics)
        
        print("æå–çš„å…³é”®ä¿¡æ¯:")
        for topic, info_points in key_info.items():
            print(f"  {topic}:")
            for point in info_points:
                print(f"    - {point}")
        
        return {"relevance": analysis_result.get('relevance', 0), "insights": len(insights)}
    
    def test_synthesis_agent(self):
        """æµ‹è¯•åˆæˆä»£ç†"""
        print("ğŸ“ æµ‹è¯•ç ”ç©¶æ‘˜è¦ç”ŸæˆåŠŸèƒ½...")
        
        # æ¨¡æ‹Ÿç ”ç©¶å‘ç°
        findings = [
            {
                "title": "AIåœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­çš„åº”ç”¨",
                "content": "äººå·¥æ™ºèƒ½åœ¨åŒ»å­¦å½±åƒè¯Šæ–­ä¸­å±•ç°å‡ºå·¨å¤§æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨Xå…‰ã€CTå’ŒMRIå›¾åƒçš„åˆ†ææ–¹é¢ã€‚",
                "url": "https://example.com/ai-medical-imaging"
            },
            {
                "title": "æœºå™¨å­¦ä¹ åŠ é€Ÿè¯ç‰©å‘ç°",
                "content": "æœºå™¨å­¦ä¹ ç®—æ³•æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜è¯ç‰©å‘ç°è¿‡ç¨‹ï¼Œæ˜¾è‘—ç¼©çŸ­æ–°è¯å¼€å‘å‘¨æœŸã€‚",
                "url": "https://example.com/ml-drug-discovery"
            },
            {
                "title": "ä¸ªæ€§åŒ–åŒ»ç–—çš„AIåº”ç”¨",
                "content": "åŸºäºAIçš„ä¸ªæ€§åŒ–åŒ»ç–—æ­£åœ¨æˆä¸ºç°å®ï¼Œé€šè¿‡åˆ†ææ‚£è€…çš„åŸºå› å’ŒåŒ»ç–—å†å²æä¾›å®šåˆ¶åŒ–æ²»ç–—ã€‚",
                "url": "https://example.com/ai-personalized-medicine"
            }
        ]
        
        query = "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ç°çŠ¶"
        
        # ç”Ÿæˆç ”ç©¶æ‘˜è¦
        summary_result = self.synthesis_agent.generate_research_summary(query, findings)
        
        print(f"ç ”ç©¶æŸ¥è¯¢: {query}")
        print(f"ç ”ç©¶ID: {summary_result.get('research_id', 'æ— ID')}")
        print(f"æ‘˜è¦:\n{summary_result.get('summary', 'æ— æ‘˜è¦')}")
        
        key_points = summary_result.get('key_points', [])
        print(f"\nå…³é”®ç‚¹ ({len(key_points)}æ¡):")
        for i, point in enumerate(key_points, 1):
            print(f"  {i}. {point}")
        
        sources = summary_result.get('sources', [])
        print(f"\næ¥æº ({len(sources)}æ¡):")
        for i, source in enumerate(sources, 1):
            print(f"  {i}. {source.get('title', 'æ— æ ‡é¢˜')} - {source.get('url', 'æ— URL')}")
        
        # æµ‹è¯•ç ”ç©¶å­˜å‚¨
        print(f"\nğŸ’¾ æµ‹è¯•ç ”ç©¶ç»“æœå­˜å‚¨...")
        store_result = self.synthesis_agent.store_research(query, summary_result)
        
        if store_result.get('success'):
            print(f"ç ”ç©¶ç»“æœå·²å­˜å‚¨ï¼ŒID: {store_result.get('research_id')}")
        else:
            print(f"å­˜å‚¨å¤±è´¥: {store_result.get('error')}")
        
        # æµ‹è¯•ç›¸å…³ç ”ç©¶æŸ¥æ‰¾
        print(f"\nğŸ” æµ‹è¯•ç›¸å…³ç ”ç©¶æŸ¥æ‰¾...")
        related_query = "AIåŒ»ç–—åº”ç”¨"
        related_research = self.synthesis_agent.find_related_research(related_query, limit=2)
        
        print(f"ç›¸å…³ç ”ç©¶æŸ¥è¯¢: {related_query}")
        print(f"æ‰¾åˆ° {len(related_research)} æ¡ç›¸å…³ç ”ç©¶:")
        for i, research in enumerate(related_research, 1):
            print(f"  {i}. {research.get('query', 'æ— æŸ¥è¯¢')}")
            print(f"     ç›¸å…³æ€§åˆ†æ•°: {research.get('relevance_score', 0)}")
        
        return {"summary_length": len(summary_result.get('summary', '')), "key_points": len(key_points)}
    
    def test_knowledge_base(self):
        """æµ‹è¯•çŸ¥è¯†åº“åŠŸèƒ½"""
        print("ğŸ“š æµ‹è¯•çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½...")
        
        # æµ‹è¯•æ·»åŠ æ–‡æœ¬å†…å®¹
        print("ğŸ“ æµ‹è¯•æ·»åŠ æ–‡æœ¬å†…å®¹...")
        test_content = """
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒæ¨¡ä»¿äººè„‘ç¥ç»ç½‘ç»œçš„ç»“æ„å’ŒåŠŸèƒ½ã€‚
        æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„é«˜çº§ç‰¹å¾è¡¨ç¤ºã€‚
        
        ä¸»è¦ç‰¹ç‚¹ï¼š
        1. å¤šå±‚ç»“æ„ï¼šä½¿ç”¨å¤šä¸ªéšè—å±‚æ¥æå–å¤æ‚ç‰¹å¾
        2. è‡ªåŠ¨ç‰¹å¾å­¦ä¹ ï¼šæ— éœ€æ‰‹åŠ¨è®¾è®¡ç‰¹å¾ï¼Œå¯ä»¥è‡ªåŠ¨å­¦ä¹ 
        3. å¤§æ•°æ®å¤„ç†ï¼šèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†
        4. å¼ºå¤§çš„è¡¨ç¤ºèƒ½åŠ›ï¼šå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»
        
        åº”ç”¨é¢†åŸŸï¼š
        - è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒè¯†åˆ«ã€ç‰©ä½“æ£€æµ‹
        - è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»
        - è¯­éŸ³è¯†åˆ«ï¼šè¯­éŸ³è½¬æ–‡æœ¬ã€è¯­éŸ³åˆæˆ
        - æ¨èç³»ç»Ÿï¼šä¸ªæ€§åŒ–æ¨èã€å†…å®¹è¿‡æ»¤
        """
        
        add_result = self.knowledge_base.add_text_content(
            content=test_content,
            title="æ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯†",
            category="machine_learning",
            tags=["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "äººå·¥æ™ºèƒ½"]
        )
        
        if add_result.get('success'):
            print(f"âœ… æˆåŠŸæ·»åŠ æ–‡æœ¬å†…å®¹ï¼Œæ–‡æ¡£ID: {add_result.get('doc_id')}")
            print(f"   åˆ†å—æ•°é‡: {add_result.get('chunk_count')}")
        else:
            print(f"âŒ æ·»åŠ æ–‡æœ¬å†…å®¹å¤±è´¥: {add_result.get('error')}")
        
        # æµ‹è¯•çŸ¥è¯†åº“æœç´¢
        print(f"\nğŸ” æµ‹è¯•çŸ¥è¯†åº“æœç´¢...")
        search_results = self.knowledge_base.search_knowledge("æ·±åº¦å­¦ä¹ çš„åº”ç”¨", top_k=3)
        
        print(f"æœç´¢æŸ¥è¯¢: æ·±åº¦å­¦ä¹ çš„åº”ç”¨")
        print(f"æ‰¾åˆ° {len(search_results)} æ¡ç»“æœ:")
        for i, result in enumerate(search_results, 1):
            print(f"  {i}. ç›¸å…³æ€§åˆ†æ•°: {result.score:.3f}")
            print(f"     å†…å®¹é¢„è§ˆ: {result.page_content[:100]}...")
            print(f"     åˆ†ç±»: {result.metadata.get('category', 'æœªçŸ¥')}")
            print(f"     æ ‡ç­¾: {result.metadata.get('tags', [])}")
        
        # æµ‹è¯•åˆ—å‡ºæ–‡æ¡£
        print(f"\nğŸ“‹ æµ‹è¯•æ–‡æ¡£åˆ—è¡¨...")
        documents = self.knowledge_base.list_documents()
        
        print(f"çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£ ({len(documents)}æ¡):")
        for i, doc in enumerate(documents, 1):
            print(f"  {i}. {doc.get('title', 'æ— æ ‡é¢˜')}")
            print(f"     åˆ†ç±»: {doc.get('category', 'æœªçŸ¥')}")
            print(f"     æ ‡ç­¾: {doc.get('tags', [])}")
            print(f"     å—æ•°: {doc.get('chunk_count', 0)}")
        
        # æµ‹è¯•è·å–ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯...")
        stats = self.knowledge_base.get_statistics()
        
        print("çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}")
        print(f"  æ€»å—æ•°: {stats.get('total_chunks', 0)}")
        print(f"  æ€»å†…å®¹é•¿åº¦: {stats.get('total_content_length', 0)}")
        print(f"  åˆ†ç±»: {stats.get('categories', {})}")
        print(f"  æ ‡ç­¾æ•°: {stats.get('unique_tags', 0)}")
        
        return {"documents": len(documents), "search_results": len(search_results)}
    
    def test_complete_research_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„ç ”ç©¶å·¥ä½œæµç¨‹"""
        print("ğŸ”„ æµ‹è¯•å®Œæ•´ç ”ç©¶å·¥ä½œæµç¨‹...")
        
        # 1. å®šä¹‰ç ”ç©¶ä¸»é¢˜
        research_topic = "åŒºå—é“¾æŠ€æœ¯åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­çš„åº”ç”¨"
        print(f"ç ”ç©¶ä¸»é¢˜: {research_topic}")
        
        # 2. æ‰§è¡Œç½‘ç»œæœç´¢
        print(f"\nğŸ” æ­¥éª¤1: ç½‘ç»œæœç´¢...")
        search_results = self.search_agent.search_web(research_topic, max_results=3)
        print(f"æ‰¾åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")
        
        # 3. è·å–å’Œåˆ†ææ–‡æ¡£
        print(f"\nğŸ“„ æ­¥éª¤2: æ–‡æ¡£è·å–å’Œåˆ†æ...")
        analyzed_findings = []
        
        for i, result in enumerate(search_results[:2]):  # åªå¤„ç†å‰2ä¸ªç»“æœä»¥èŠ‚çœæ—¶é—´
            print(f"  å¤„ç†ç»“æœ {i+1}: {result.get('title', 'æ— æ ‡é¢˜')}")
            
            # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹ï¼ˆåœ¨çœŸå®ç¯å¢ƒä¸­ä¼šè·å–å®é™…å†…å®¹ï¼‰
            mock_content = f"""
            {result.get('title', '')}
            
            {result.get('snippet', '')}
            
            åŒºå—é“¾æŠ€æœ¯ä¸ºä¾›åº”é“¾ç®¡ç†æä¾›äº†é€æ˜åº¦ã€å¯è¿½æº¯æ€§å’Œå®‰å…¨æ€§ã€‚
            é€šè¿‡åˆ†å¸ƒå¼è´¦æœ¬æŠ€æœ¯ï¼Œä¼ä¸šå¯ä»¥å®æ—¶è·Ÿè¸ªäº§å“ä»åŸææ–™åˆ°æœ€ç»ˆæ¶ˆè´¹è€…çš„æ•´ä¸ªæµç¨‹ã€‚
            ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼šå‡å°‘æ¬ºè¯ˆã€æé«˜æ•ˆç‡ã€é™ä½æˆæœ¬ã€å¢å¼ºä¿¡ä»»ã€‚
            """
            
            # åˆ†æå†…å®¹
            analysis = self.analysis_agent.analyze_content(mock_content, research_topic)
            
            finding = {
                "title": result.get('title', ''),
                "url": result.get('url', ''),
                "content": mock_content,
                "analysis": analysis,
                "relevance": analysis.get('relevance', 0)
            }
            
            analyzed_findings.append(finding)
            print(f"    ç›¸å…³æ€§åˆ†æ•°: {analysis.get('relevance', 0):.2f}")
        
        # 4. ç”Ÿæˆç ”ç©¶æ‘˜è¦
        print(f"\nğŸ“ æ­¥éª¤3: ç”Ÿæˆç ”ç©¶æ‘˜è¦...")
        summary_result = self.synthesis_agent.generate_research_summary(
            research_topic, analyzed_findings
        )
        
        print(f"ç ”ç©¶æ‘˜è¦å·²ç”Ÿæˆ:")
        print(f"  æ‘˜è¦é•¿åº¦: {len(summary_result.get('summary', ''))}")
        print(f"  å…³é”®ç‚¹æ•°é‡: {len(summary_result.get('key_points', []))}")
        print(f"  æ¥æºæ•°é‡: {len(summary_result.get('sources', []))}")
        
        # æ˜¾ç¤ºæ‘˜è¦å†…å®¹
        if summary_result.get('summary'):
            print(f"\næ‘˜è¦å†…å®¹:")
            print(summary_result['summary'][:300] + "..." if len(summary_result['summary']) > 300 else summary_result['summary'])
        
        # 5. å­˜å‚¨ç ”ç©¶ç»“æœ
        print(f"\nğŸ’¾ æ­¥éª¤4: å­˜å‚¨ç ”ç©¶ç»“æœ...")
        store_result = self.synthesis_agent.store_research(research_topic, summary_result)
        
        if store_result.get('success'):
            print(f"âœ… ç ”ç©¶ç»“æœå·²å­˜å‚¨ï¼ŒID: {store_result.get('research_id')}")
        else:
            print(f"âŒ å­˜å‚¨å¤±è´¥: {store_result.get('error')}")
        
        # 6. æ·»åŠ åˆ°çŸ¥è¯†åº“
        print(f"\nğŸ“š æ­¥éª¤5: æ·»åŠ åˆ°çŸ¥è¯†åº“...")
        kb_result = self.knowledge_base.add_text_content(
            content=summary_result.get('summary', ''),
            title=f"ç ”ç©¶æŠ¥å‘Š: {research_topic}",
            category="research_report",
            tags=["åŒºå—é“¾", "ä¾›åº”é“¾", "ç ”ç©¶æŠ¥å‘Š"],
            source="research_workflow"
        )
        
        if kb_result.get('success'):
            print(f"âœ… ç ”ç©¶æŠ¥å‘Šå·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼Œæ–‡æ¡£ID: {kb_result.get('doc_id')}")
        else:
            print(f"âŒ æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {kb_result.get('error')}")
        
        # 7. æŸ¥æ‰¾ç›¸å…³ç ”ç©¶
        print(f"\nğŸ”— æ­¥éª¤6: æŸ¥æ‰¾ç›¸å…³ç ”ç©¶...")
        related_research = self.synthesis_agent.find_related_research("åŒºå—é“¾ä¾›åº”é“¾", limit=2)
        
        print(f"æ‰¾åˆ° {len(related_research)} æ¡ç›¸å…³ç ”ç©¶:")
        for i, research in enumerate(related_research, 1):
            print(f"  {i}. {research.get('query', 'æ— æŸ¥è¯¢')}")
            if research.get('relevance_score'):
                print(f"     ç›¸å…³æ€§: {research.get('relevance_score'):.2f}")
        
        return {
            "search_results": len(search_results),
            "analyzed_findings": len(analyzed_findings),
            "summary_generated": bool(summary_result.get('summary')),
            "stored_successfully": store_result.get('success', False),
            "added_to_kb": kb_result.get('success', False)
        }
    
    def show_test_summary(self, results: List[Dict[str, Any]]):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)
        
        total_tests = len(results)
        successful_tests = sum(1 for r in results if r["status"] == "success")
        failed_tests = total_tests - successful_tests
        total_time = sum(r.get("time", 0) for r in results if "time" in r)
        
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸ: {successful_tests} âœ…")
        print(f"å¤±è´¥: {failed_tests} âŒ")
        print(f"æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        print(f"\nè¯¦ç»†ç»“æœ:")
        for result in results:
            status_icon = "âœ…" if result["status"] == "success" else "âŒ"
            test_name = result["test"]
            
            if result["status"] == "success":
                time_info = f" ({result.get('time', 0):.2f}s)" if "time" in result else ""
                print(f"  {status_icon} {test_name}{time_info}")
            else:
                error_info = result.get("error", "æœªçŸ¥é”™è¯¯")
                print(f"  {status_icon} {test_name} - {error_info}")
        
        # æ˜¾ç¤ºæ€§èƒ½å»ºè®®
        if total_time > 30:
            print(f"\nğŸ’¡ æ€§èƒ½å»ºè®®:")
            print(f"   æµ‹è¯•è€—æ—¶è¾ƒé•¿ï¼Œå»ºè®®æ£€æŸ¥APIå“åº”é€Ÿåº¦å’Œç½‘ç»œè¿æ¥")
        
        if failed_tests > 0:
            print(f"\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
            print(f"   1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")
            print(f"   2. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
            print(f"   3. æ£€æŸ¥ä¾èµ–åº“æ˜¯å¦æ­£ç¡®å®‰è£…")
            print(f"   4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—")
    
    def create_sample_mcp_config(self):
        """åˆ›å»ºç¤ºä¾‹MCPé…ç½®æ–‡ä»¶"""
        print("\nğŸ“„ åˆ›å»ºç¤ºä¾‹MCPé…ç½®æ–‡ä»¶...")
        
        config = {
            "mcpServers": {
                "intelligent-research-assistant": {
                    "command": "python",
                    "args": [str(project_root / "src" / "main.py")],
                    "env": {
                        "TRANSPORT": "stdio",
                        "OPENAI_API_KEY": "your_openai_api_key_here",
                        "GROQ_API_KEY": "your_groq_api_key_here",
                        "SEARCH_API_KEY": "your_search_api_key_here",
                        "EXA_API_KEY": "your_exa_api_key_here",
                        "LOCAL_STORAGE_PATH": "./data/vector_store",
                        "KB_INDEX_PATH": "./data/knowledge_base_index.json"
                    }
                }
            }
        }
        
        config_path = project_root / "claude_desktop_config_example.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… MCPé…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_path}")
        print(f"   å°†æ­¤é…ç½®æ·»åŠ åˆ°Claude Desktopé…ç½®æ–‡ä»¶ä¸­ä»¥ä½¿ç”¨MCPæœåŠ¡å™¨")
        
        return str(config_path)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ MCP æœåŠ¡å™¨æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥åŸºæœ¬ç¯å¢ƒ
        print("ğŸ”§ æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        if sys.version_info < (3, 8):
            print("âŒ éœ€è¦Python 3.8æˆ–æ›´é«˜ç‰ˆæœ¬")
            return
        
        print(f"âœ… Pythonç‰ˆæœ¬: {sys.version}")
        
        # æ£€æŸ¥å¿…è¦çš„ç›®å½•
        required_dirs = ["src", "src/agents", "src/rag", "src/tools"]
        for dir_path in required_dirs:
            full_path = project_root / dir_path
            if not full_path.exists():
                print(f"âŒ ç¼ºå°‘å¿…è¦ç›®å½•: {dir_path}")
                return
        
        print("âœ… ç›®å½•ç»“æ„æ£€æŸ¥é€šè¿‡")
        
        # åˆ›å»ºæ¼”ç¤ºè¿è¡Œå™¨
        demo_runner = DemoTestRunner()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        demo_runner.run_all_tests()
        
        # åˆ›å»ºç¤ºä¾‹é…ç½®æ–‡ä»¶
        demo_runner.create_sample_mcp_config()
        
        print(f"\nğŸ‰ æ¼”ç¤ºæµ‹è¯•å®Œæˆï¼")
        print(f"å¦‚éœ€åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œè¯·ï¼š")
        print(f"1. è®¾ç½®æ­£ç¡®çš„APIå¯†é’¥åœ¨.envæ–‡ä»¶ä¸­")
        print(f"2. å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        print(f"3. é…ç½®Claude Desktopä»¥ä½¿ç”¨æ­¤MCPæœåŠ¡å™¨")
        print(f"4. è¿è¡Œ: python src/main.py")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  æ¼”ç¤ºæµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()